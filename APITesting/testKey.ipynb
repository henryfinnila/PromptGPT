{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import openai\n",
    "#import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: error loading model: llama_model_loader: failed to load model from C:/Users/hpfin/OneDrive/Desktop/Time Series/Mines/Junior/LLM/FinalProject/whisper-large-v3.bin\n",
      "\n",
      "llama_model_load_from_file_impl: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: C:/Users/hpfin/OneDrive/Desktop/Time Series/Mines/Junior/LLM/FinalProject/whisper-large-v3.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mC:/Users/hpfin/OneDrive/Desktop/Time Series/Mines/Junior/LLM/FinalProject/whisper-large-v3.bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize the model; note that the library must support your file format.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m llm = \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model\u001b[39m():\n\u001b[32m     10\u001b[39m     prompt = \u001b[33m\"\u001b[39m\u001b[33mExplain how to integrate a local model in VSCode.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hpfin\\OneDrive\\Desktop\\Time Series\\Mines\\Junior\\LLM\\FinalProject\\venv\\Lib\\site-packages\\llama_cpp\\llama.py:372\u001b[39m, in \u001b[36mLlama.__init__\u001b[39m\u001b[34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_path):\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    370\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[38;5;28mself\u001b[39m._stack.enter_context(\n\u001b[32m    371\u001b[39m     contextlib.closing(\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[43minternals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     )\n\u001b[32m    378\u001b[39m )\n\u001b[32m    380\u001b[39m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer_ = tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hpfin\\OneDrive\\Desktop\\Time Series\\Mines\\Junior\\LLM\\FinalProject\\venv\\Lib\\site-packages\\llama_cpp\\_internals.py:56\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, path_model, params, verbose)\u001b[39m\n\u001b[32m     51\u001b[39m     model = llama_cpp.llama_load_model_from_file(\n\u001b[32m     52\u001b[39m         \u001b[38;5;28mself\u001b[39m.path_model.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m.params\n\u001b[32m     53\u001b[39m     )\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m vocab = llama_cpp.llama_model_get_vocab(model)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Failed to load model from file: C:/Users/hpfin/OneDrive/Desktop/Time Series/Mines/Junior/LLM/FinalProject/whisper-large-v3.bin"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Update the model_path with the full (or relative) path to your .llamafile\n",
    "model_path = \"C:/Users/hpfin/OneDrive/Desktop/Time Series/Mines/Junior/LLM/FinalProject/whisper-large-v3.bin\"\n",
    "\n",
    "# Initialize the model; note that the library must support your file format.\n",
    "llm = Llama(model_path=model_path)\n",
    "\n",
    "def test_model():\n",
    "    prompt = \"Explain how to integrate a local model in VSCode.\"\n",
    "    # Call the model with your prompt and adjust parameters as needed.\n",
    "    response = llm(prompt, max_tokens=100, temperature=0.7)\n",
    "    # The format of the response may differ; here we assume it returns a dict with a 'choices' key.\n",
    "    # For example purposes, we simply print the full response.\n",
    "    print(\"Response from the model:\")\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import unittest\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Load environment variables from a .env file (if using one)\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key. You can also hard-code it here temporarily.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# For testing purposes only (do NOT commit or share hard-coded API keys):\n",
    "# openai.api_key = \"sk-YourActualKeyHere\"\n",
    "\n",
    "def test_o3_mini_high() -> str:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API using the legacy completions endpoint with the 'o3-mini-high' engine,\n",
    "    using a prompt that describes the assignment to implement a domino chain solver.\n",
    "    Returns the candidate solution code as a string.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"o3-mini-high\",  # using the legacy 'engine' parameter\n",
    "        prompt=\"\"\"# Assignment: Maximum Domino Chain\n",
    "\n",
    "**Problem Description**\n",
    "You are given a list of domino pieces, where each domino is represented as a tuple of two integers (e.g., (a, b)). Your task is to write a Python function that finds a domino chain using as many pieces as possible from the input list. A domino chain is defined as a sequence of domino pieces where the second number of one piece matches the first number of the next piece. Domino pieces may be flipped (i.e., (a, b) can be used as (b, a)) to allow for chaining.\n",
    "\n",
    "**Requirements**\n",
    "1. Implement a function max_domino_chain(dominoes: List[Tuple[int, int]]) -> List[Tuple[int, int]].\n",
    "2. The function should return a valid domino chain (as a list of domino pieces) that uses the maximum number of dominoes possible from the input.\n",
    "3. Each domino piece can be used at most once in the chain.\n",
    "4. Domino pieces may be flipped if necessary.\n",
    "5. If multiple chains have the maximum length, you may return any one of them.\n",
    "6. If no domino pieces are provided or if no valid chain can be formed, return an empty list.\n",
    "7. Assume the input size will not exceed 15 dominoes.\n",
    "\n",
    "**Example**\n",
    "Example input: dominoes = [(1, 2), (3, 1), (2, 3)]\n",
    "One valid chain (after flipping as needed) is: [(3, 1), (1, 2), (2, 3)]\n",
    "Explanation: 3 -> 1 connects to 1 -> 2, which connects to 2 -> 3.\n",
    "\"\"\",\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    candidate_code = response.choices[0].text.strip()\n",
    "    print(\"Response from o3-mini-high:\")\n",
    "    print(candidate_code)\n",
    "    return candidate_code\n",
    "\n",
    "def grade_o3_mini_high(candidate_code: str) -> int:\n",
    "    \"\"\"\n",
    "    Grades the candidate solution code by performing the following:\n",
    "      1. Writes candidate_code to a file named solution.py.\n",
    "      2. Loads and runs the testsuite from testsuite.py using unittest.\n",
    "      3. Returns an integer grade (0 to 50) corresponding to the number of tests passed.\n",
    "    \"\"\"\n",
    "    # Write the candidate solution to solution.py\n",
    "    with open(\"solution.py\", \"w\") as f:\n",
    "        f.write(candidate_code)\n",
    "    \n",
    "    # Discover and run the testsuite from testsuite.py.\n",
    "    loader = unittest.TestLoader()\n",
    "    # Discover tests in the current directory with filename pattern testsuite.py\n",
    "    suite = loader.discover('.', pattern='testsuite.py')\n",
    "    \n",
    "    # Capture the test output into an in-memory stream (so that it doesn't clutter the console)\n",
    "    stream = io.StringIO()\n",
    "    runner = unittest.TextTestRunner(stream=stream, verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Calculate score: each test passed (i.e. not failing or erroring) is one point.\n",
    "    tests_run = result.testsRun\n",
    "    failures = len(result.failures)\n",
    "    errors = len(result.errors)\n",
    "    passed = tests_run - failures - errors\n",
    "    \n",
    "    # Optionally, print the test output for debugging:\n",
    "    print(\"Test output:\\n\" + stream.getvalue())\n",
    "    print(f\"Passed {passed} out of {tests_run} tests.\")\n",
    "    \n",
    "    return passed\n",
    "\n",
    "def main():\n",
    "    grades = []  # empty list to store grades for each iteration\n",
    "    iterations = 300\n",
    "    for i in range(iterations):\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        candidate_code = test_o3_mini_high()\n",
    "        grade = grade_o3_mini_high(candidate_code)\n",
    "        grades.append(grade)\n",
    "        # Optional: Add a sleep to avoid hitting API rate limits.\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"\\nFinal Grades:\")\n",
    "    print(grades)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
